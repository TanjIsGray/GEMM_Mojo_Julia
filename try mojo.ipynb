{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts Copyright (c) Modular Inc extracted from\n",
    "    https://github.com/modularml/mojo/blob/main/examples/matmul.mojo\n",
    "\n",
    "for fair and appropriate use here in a Jupyter notebook\n",
    "\n",
    "Used the Modular Mojo (Mojo) kernel current on 15th Apr 2024 for test results discussed in the ReadMe\n",
    "\n",
    "The Mojo benchmark is intended as a learning tool so the Matrix type is defined from basics, rather than built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias type = DType.float32\n",
    "\n",
    "struct Matrix[rows: Int, cols: Int]:\n",
    "    var data: DTypePointer[type]\n",
    "\n",
    "    # Initialize zeroeing all values\n",
    "    fn __init__(inout self):\n",
    "        self.data = DTypePointer[type].alloc(rows * cols)\n",
    "        memset_zero(self.data, rows * cols)\n",
    "\n",
    "    # Initialize taking a pointer, don't set any elements\n",
    "    fn __init__(inout self, data: DTypePointer[type]):\n",
    "        self.data = data\n",
    "\n",
    "    # Initialize with random values\n",
    "    @staticmethod\n",
    "    fn rand() -> Self:\n",
    "        var data = DTypePointer[type].alloc(rows * cols)\n",
    "        random.rand(data, rows * cols)\n",
    "        return Self(data)\n",
    "\n",
    "    fn __getitem__(self, y: Int, x: Int) -> Scalar[type]:\n",
    "        return self.load[1](y, x)\n",
    "\n",
    "    fn __setitem__(self, y: Int, x: Int, val: Scalar[type]):\n",
    "        self.store[1](y, x, val)\n",
    "\n",
    "    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[type, nelts]:\n",
    "        return self.data.load[width=nelts](y * self.cols + x)\n",
    "\n",
    "    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n",
    "        return self.data.store[width=nelts](y * self.cols + x, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive version allows the A and B looks like it should be slow but the optimiser will end up making it x faster than the CmnDot approach.  This probably reflects some optimization using AVX that is recognized for the naive case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that C, A, and B have types.\n",
    "fn matmul_naive(C: Matrix, A: Matrix, B: Matrix):\n",
    "    for m in range(C.rows):\n",
    "        for k in range(A.cols):\n",
    "            for n in range(C.cols):\n",
    "                C[m, n] += A[m, k] * B[k, n]\n",
    "\n",
    "fn matmul_CmnDot(C: Matrix, A: Matrix, B: Matrix):\n",
    "    var Cmn: Float32 = 0\n",
    "    for m in range(C.rows):\n",
    "        for n in range(C.cols):\n",
    "            Cmn = 0\n",
    "            for k in range(A.cols):\n",
    "                Cmn += A[m, k] * B[k, n]\n",
    "            C[m,n] = Cmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mojo bench creates and frees the C, A, B arrays every trial.\n",
    "This reflects the Mojo philosophy of tightly designed minimal data lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias M = 1024\n",
    "alias N = 1024\n",
    "alias K = 1024\n",
    "\n",
    "import benchmark\n",
    "\n",
    "@always_inline\n",
    "fn bench[\n",
    "    func: fn (Matrix, Matrix, Matrix) -> None]():\n",
    "    var C = Matrix[M, N]()\n",
    "    var A = Matrix[M, K].rand()\n",
    "    var B = Matrix[K, N].rand()\n",
    "\n",
    "    @always_inline\n",
    "    @parameter\n",
    "    fn test_fn():\n",
    "        _ = func(C, A, B)\n",
    "\n",
    "    var secs = benchmark.run[test_fn](max_runtime_secs=1).mean()\n",
    "\n",
    "    print (C[0,0], \", \", C[1,0], \", \", C[0,1000], \", \", C[1000,0])\n",
    "    A.data.free()\n",
    "    B.data.free()\n",
    "    C.data.free()\n",
    "\n",
    "# it is conventional to count \"+=\" as 2 FLOPs even though the fused multiply-add is pipelined as one op.\n",
    "\n",
    "    var gflops = ((2 * M * N * K) / secs) / 1e9\n",
    "\n",
    "    print(gflops, \"GFLOP/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250.37939453125 ,  1230.5911865234375 ,  1314.1461181640625 ,  1216.9930419921875\n",
      "4.2374179621743062 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "bench[matmul_naive]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product in the inner loop, is slower than naive.\n",
    "Julia runs at identical speed probably making the same optimization decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.63015747070312 ,  254.29837036132812 ,  249.54191589355469 ,  255.52023315429688\n",
      "1.5286133724283093 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "bench[matmul_CmnDot]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise that Mojo has us rewrite the function to vectorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the code by using the builtin vectorize function\n",
    "from algorithm import vectorize\n",
    "\n",
    "alias nelts = simdwidthof[type]() * 2\n",
    "\n",
    "fn matmul_vectorized_1(C: Matrix, A: Matrix, B: Matrix):\n",
    "    for m in range(C.rows):\n",
    "        for k in range(A.cols):\n",
    "            @parameter\n",
    "            fn dot[nelts: Int](n: Int):\n",
    "                C.store(m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n))\n",
    "            vectorize[dot, nelts, size = C.cols]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my 8 core Intel Gen 11 this goes 6x faster than the naive version, around 27 GFLOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9920.544921875 ,  10347.28125 ,  10458.8271484375 ,  10580.2578125\n",
      "27.139883246045727 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "bench[matmul_vectorized_1]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelizing is not a large change to the code, but it converts the outer For loop to a function call for one row and then iterates the rows in parallel.\n",
    "\n",
    "My 8 core Intel Gen 11 ends up a little more than 4x faster when parallel, at around 116 GFLOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the code by using the builtin parallelize function\n",
    "from algorithm import parallelize\n",
    "\n",
    "fn matmul_parallelized(C: Matrix, A: Matrix, B: Matrix):\n",
    "    @parameter\n",
    "    fn calc_row(m: Int):\n",
    "        for k in range(A.cols):\n",
    "            @parameter\n",
    "            fn dot[nelts : Int](n : Int):\n",
    "                C.store[nelts](m,n, C.load[nelts](m,n) + A[m,k] * B.load[nelts](k,n))\n",
    "            vectorize[dot, nelts, size = C.cols]()\n",
    "    parallelize[calc_row](C.rows, C.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25836.423828125 ,  25676.44921875 ,  27097.4375 ,  26004.072265625\n",
      "116.38407916540572 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "bench[matmul_parallelized]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
