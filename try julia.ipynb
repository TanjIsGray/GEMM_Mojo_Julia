{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Parts` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Parts` not defined\n"
     ]
    }
   ],
   "source": [
    "Parts Copyright (c) Chris Elroy extracted from\n",
    "    https://github.com/JuliaSIMD/LoopVectorization.jl#usage\n",
    "\n",
    "for fair and appropriate use here in a Jupyter notebook as per the MIT license.\n",
    "\n",
    "Used the Julia release channel kernel current on 15th Apr 2024\n",
    "\n",
    "The Julia is native compiled and the vector/matrix/tensors you might want are built in,\n",
    "  so we do not need to roll any types by hand.\n",
    "We can pull in some packages for optimizations.\n",
    "This reflects the maturity of the Julia ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "import BenchmarkTools\n",
    "\n",
    "Pkg.add(\"LoopVectorization\")\n",
    "import LoopVectorization\n",
    "\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "import LinearAlgebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the general test parameters.\n",
    "Unlike the Mojo benchmark we do not create and free the matrices at every test.\n",
    "We could, but neither version measures the time for memory management and\n",
    "so there is no need to create and free.\n",
    "C will be overwritten for each test.  A and B remain constant after random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "M = 1024\n",
    "N = 1024\n",
    "K = 1024\n",
    "\n",
    "C = zeros(Float32, M, N)\n",
    "A = rand(Float32, M, K)\n",
    "B = rand(Float32, K, N)\n",
    "\n",
    "# it is conventional to count \"+=\" as 2 FLOPs even though the fused multiply-add is pipelined as one op.\n",
    "\n",
    "FLOPcount = 2 * M * N * K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive, CmnDot, and vectorized versions are all together.\n",
    "\n",
    "They are all short.  The vectorized (avx) version just has an @turbo macro added and takes away the @inbounds and @fastmath, it is otherwise the same as the CmnDot version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mygemmavx! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LoopVectorization\n",
    "\n",
    "# adding a naive version similar to Mojo sample, to see what the optimizer does\n",
    "\n",
    "function mygemm_naive!(C, A, B)\n",
    "    @inbounds @fastmath for m ∈ axes(A,1), k ∈ axes(A,2)\n",
    "        for n ∈ axes(B,2)\n",
    "            C[m,n] += A[m,k] * B[k,n]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# citation to https://github.com/JuliaSIMD/LoopVectorization.jl#usage\n",
    "\n",
    "function mygemm!(C, A, B)\n",
    "    @inbounds @fastmath for m ∈ axes(A,1), n ∈ axes(B,2)\n",
    "        Cmn = zero(eltype(C))\n",
    "        for k ∈ axes(A,2)\n",
    "            Cmn += A[m,k] * B[k,n]\n",
    "        end\n",
    "        C[m,n] = Cmn\n",
    "    end\n",
    "end\n",
    "\n",
    "# just add turbo!\n",
    "\n",
    "function mygemm_naiveavx!(C, A, B)\n",
    "    @turbo for m ∈ axes(A,1), k ∈ axes(A,2)\n",
    "        for n ∈ axes(B,2)\n",
    "            C[m,n] += A[m,k] * B[k,n]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function mygemmavx!(C, A, B)\n",
    "    @turbo for m ∈ axes(A,1), n ∈ axes(B,2)\n",
    "        Cmn = zero(eltype(C))\n",
    "        for k ∈ axes(A,2)\n",
    "            Cmn += A[m,k] * B[k,n]\n",
    "        end\n",
    "        C[m,n] = Cmn\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive version is much slower than the CmnDot.\n",
    "\n",
    "But when @turbo is used to vectorize them they become almost equally optimized\n",
    "at around 81 GFLOPs, which is a more then 50x improvement (and 3x faster than the Mojo vectorized version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive: 6.993728659 secs, 0.30705847377085094 GFLOPs\n",
      "CmnDot: 1.444738797 secs, 1.4864165428790654 GFLOPs\n",
      "Naive turbo: 0.024527639 secs, 87.55362258878648 GFLOPs\n",
      "CmnDot turbo: 0.023422773 secs, 91.6835785412769 GFLOPs"
     ]
    }
   ],
   "source": [
    "elapsed = @belapsed mygemm_naive!(C, B, A)\n",
    "\n",
    "println(\"Naive: \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")\n",
    "\n",
    "elapsed = @belapsed mygemm!(C, B, A)\n",
    "\n",
    "println(\"CmnDot: \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")\n",
    "\n",
    "elapsed = @belapsed mygemm_naiveavx!(C, B, A)\n",
    "\n",
    "println(\"Naive turbo: \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")\n",
    "\n",
    "elapsed = @belapsed mygemmavx!(C, B, A)\n",
    "\n",
    "print(\"CmnDot turbo: \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, rather than parallelizing by hand, which is possible with about the same coding effort as Mojo, lets just use Julia ecosystem where it is already done.\n",
    "\n",
    "Import LinearAlgebra.  It comes complete with overloaded operators, so that as usual in Julia it reads just like your math textbooks would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mygemmBLAS! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function mygemmBLAS!(C, A, B)\n",
    "    C = A * B\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not playing any more.  Let's do this Julia style.\n",
    "\n",
    "My CPU has 8 cores so I select 8 threads (16 hyperthreads runs worse, not better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAS 1 thread : 0.016727589 secs, 128.37974725467012 GFLOPs\n",
      "BLAS 8 threads: 0.002861231 secs, 750.5453589731134 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "BLAS.set_num_threads(1)\n",
    "\n",
    "elapsed = @belapsed mygemmBLAS!(C, B, A)\n",
    "\n",
    "println(\"BLAS 1 thread : \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")\n",
    "\n",
    "BLAS.set_num_threads(8)\n",
    "\n",
    "elapsed = @belapsed mygemmBLAS!(C, B, A)\n",
    "\n",
    "println(\"BLAS 8 threads: \", elapsed, \" secs, \", FLOPcount / 1e9 / elapsed, \" GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "129 GFLOPs single thread is about 1.4x faster than the vectorized \"mygemm\" source.\n",
    "750 GFLOPs 8 threaded is almost 5x faster, and 5x faster than the Mojo parallel version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024×1024 Matrix{Float32}:\n",
       " 256.283  262.353  254.391  253.787  …  260.341  259.083  250.08   260.396\n",
       " 254.645  256.869  259.43   256.911     256.617  259.329  251.29   262.148\n",
       " 250.619  245.617  243.974  238.565     244.804  246.429  241.587  253.329\n",
       " 247.227  249.97   248.719  241.492     251.219  256.507  242.76   262.488\n",
       " 254.726  264.42   263.715  258.729     261.764  265.687  253.729  265.699\n",
       " 252.991  254.34   252.051  245.291  …  256.109  261.147  252.612  256.529\n",
       " 261.584  253.605  255.758  252.27      262.825  260.035  256.663  263.216\n",
       " 260.633  259.351  255.232  250.273     260.822  259.813  255.686  263.063\n",
       " 260.304  255.054  255.753  254.197     265.911  259.335  249.991  263.131\n",
       " 252.687  255.345  251.395  254.461     257.248  254.255  245.558  258.976\n",
       "   ⋮                                 ⋱    ⋮                        \n",
       " 264.168  260.992  260.542  263.356  …  265.777  266.97   259.857  273.076\n",
       " 258.738  258.47   260.913  253.198     262.282  263.603  257.795  269.07\n",
       " 258.21   258.599  259.478  253.969     262.1    265.437  251.217  260.224\n",
       " 253.69   257.727  258.508  253.959     257.773  261.707  255.474  255.676\n",
       " 256.843  261.197  255.974  254.502     255.425  256.535  252.433  261.463\n",
       " 258.685  259.053  256.335  261.201  …  260.016  261.531  254.846  264.832\n",
       " 249.1    255.731  249.213  245.846     258.716  250.237  247.308  260.758\n",
       " 254.939  258.858  259.242  253.583     263.962  264.488  255.364  262.448\n",
       " 255.364  255.977  259.456  253.66      261.513  260.64   248.943  258.091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check that the calculation really happened..\n",
    "# each value expected value is 0.25 * 1024 -> 256, with std dev 256/sqrt(1024) -> 8\n",
    "\n",
    "C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
